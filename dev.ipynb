{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40574277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f5525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: There are 4278 barcodes\n"
     ]
    }
   ],
   "source": [
    "def data_from_df(df, target_level = None, label_pipeline=None):\n",
    "    barcodes = df[\"nucleotides\"].to_list()\n",
    "     \n",
    "    if target_level:\n",
    "        species = df[target_level].to_list()\n",
    "        species = np.array(list(map(label_pipeline, species)))\n",
    "\n",
    "    print(f\"[INFO]: There are {len(barcodes)} barcodes\")\n",
    "    # Number of training samples and entire data\n",
    "    N = len(barcodes)\n",
    "\n",
    "    # Reading barcodes and labels into python list\n",
    "    labels = []\n",
    "\n",
    "    for i in range(N):\n",
    "        if len(barcodes[i]) > 0:\n",
    "            barcodes.append(barcodes[i])\n",
    "            if target_level:\n",
    "                labels.append(species[i])\n",
    "\n",
    "    sl = 660  # Max_length\n",
    "\n",
    "    nucleotide_dict = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3, \"N\": 4}\n",
    "\n",
    "    X = np.zeros((N, sl, 5), dtype=np.float32) #Can't do zeros because \n",
    "    for i in range(N):\n",
    "        j = 0\n",
    "        while j < min(sl, len(barcodes[i])):\n",
    "            k = nucleotide_dict[barcodes[i][j]]\n",
    "            X[i][j][k] = 1.0\n",
    "            j += 1\n",
    "                \n",
    "            \n",
    "    # print(X.shape, )\n",
    "    return X, np.array(labels)\n",
    "\n",
    "df = pd.read_csv(\"data/dev.csv\")\n",
    "X, labels = data_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687d1373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]),\n",
       " tensor([ 4,  0,  1,  3,  3,  3,  0,  3,  0,  1,  3,  3,  3,  0,  3,  3,  3,  3,\n",
       "          3,  2,  2,  0,  2,  1,  3,  3,  2,  0,  3,  1,  2,  2,  2,  0,  0,  3,\n",
       "          0,  0,  3,  3,  2,  2,  0,  0,  1,  3,  3,  1,  0,  3,  3,  0,  0,  2,\n",
       "          0,  0,  3,  3,  1,  3,  0,  0,  3,  3,  1,  2,  0,  2,  1,  1,  2,  0,\n",
       "          0,  1,  3,  0,  2,  2,  0,  1,  0,  3,  1,  1,  3,  2,  2,  0,  2,  1,\n",
       "          0,  3,  3,  0,  0,  3,  3,  2,  2,  0,  2,  0,  3,  2,  0,  3,  1,  0,\n",
       "          0,  0,  3,  3,  3,  0,  3,  0,  0,  3,  2,  3,  0,  0,  3,  3,  2,  3,\n",
       "          3,  0,  1,  0,  2,  1,  3,  1,  0,  3,  2,  1,  3,  3,  3,  3,  0,  3,\n",
       "          3,  0,  3,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  0,  3,  0,  2,  3,\n",
       "          0,  0,  3,  0,  1,  1,  0,  0,  3,  3,  0,  3,  0,  0,  3,  3,  2,  2,\n",
       "          0,  2,  2,  0,  3,  3,  3,  2,  2,  3,  0,  0,  3,  3,  2,  0,  3,  3,\n",
       "          0,  2,  3,  1,  1,  1,  3,  3,  3,  0,  0,  3,  0,  3,  3,  0,  2,  2,\n",
       "          0,  2,  1,  3,  1,  1,  0,  2,  0,  3,  0,  3,  0,  2,  1,  1,  3,  3,\n",
       "          1,  1,  1,  3,  1,  2,  2,  0,  3,  0,  0,  0,  1,  0,  0,  3,  0,  3,\n",
       "          0,  0,  2,  3,  3,  3,  1,  3,  2,  0,  1,  3,  3,  3,  3,  0,  1,  1,\n",
       "          3,  1,  1,  3,  2,  1,  0,  3,  3,  0,  0,  1,  3,  3,  3,  0,  1,  3,\n",
       "          0,  3,  3,  0,  2,  3,  0,  0,  2,  3,  0,  2,  3,  0,  3,  0,  2,  3,\n",
       "          0,  2,  0,  0,  0,  0,  1,  2,  2,  0,  2,  1,  3,  2,  2,  0,  0,  1,\n",
       "          0,  2,  2,  0,  3,  2,  0,  0,  1,  3,  2,  3,  3,  3,  0,  1,  1,  1,\n",
       "          0,  1,  1,  3,  3,  3,  0,  3,  1,  3,  3,  1,  3,  0,  0,  3,  0,  3,\n",
       "          1,  2,  1,  3,  1,  0,  3,  2,  2,  0,  2,  2,  0,  2,  1,  3,  3,  1,\n",
       "          3,  2,  3,  3,  2,  0,  3,  3,  3,  0,  4,  4,  4,  2,  1,  3,  0,  3,\n",
       "          3,  3,  3,  3,  3,  1,  3,  3,  3,  0,  1,  0,  1,  3,  3,  0,  2,  1,\n",
       "          0,  2,  2,  0,  0,  3,  3,  3,  1,  3,  3,  1,  0,  0,  3,  3,  3,  3,\n",
       "          0,  2,  2,  0,  2,  1,  3,  2,  3,  0,  0,  0,  3,  3,  3,  3,  0,  3,\n",
       "          3,  0,  1,  3,  0,  1,  0,  2,  3,  3,  0,  3,  3,  0,  0,  3,  0,  3,\n",
       "          0,  1,  2,  0,  3,  1,  0,  0,  1,  0,  2,  2,  0,  0,  3,  3,  0,  1,\n",
       "          0,  3,  3,  1,  2,  0,  1,  1,  2,  0,  0,  3,  0,  1,  1,  0,  3,  3,\n",
       "          0,  3,  3,  3,  2,  3,  3,  3,  2,  0,  3,  1,  3,  2,  3,  0,  2,  3,\n",
       "          0,  0,  3,  3,  0,  1,  0,  2,  1,  3,  3,  3,  0,  3,  3,  0,  1,  3,\n",
       "          3,  3,  3,  0,  3,  3,  0,  3,  1,  3,  3,  3,  0,  1,  1,  0,  2,  3,\n",
       "          0,  3,  3,  0,  2,  1,  0,  2,  2,  3,  2,  1,  3,  0,  3,  3,  0,  1,\n",
       "          3,  0,  3,  0,  3,  3,  0,  3,  3,  0, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True, False, False, False, False,  True,  True,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1) DATASET WITH CONTIGUOUS-CHUNK MASKING\n",
    "class MaskedOneHotDataset(Dataset):\n",
    "    def __init__(self, X, mask_ratio=0.15, chunk_size=4):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (N, L, 5), one-hot over {A,C,G,T,N}.\n",
    "           Padding rows must be all zeros.\n",
    "        mask_ratio: fraction of real tokens to mask\n",
    "        chunk_size: length of each contiguous masked span\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X)  # (N, L, 5)\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].clone()  # (L, 5)\n",
    "        L = x.size(0)\n",
    "\n",
    "        # 1) Padding mask: 1 for real tokens (sum>0), 0 for padding (zero-vector)\n",
    "        att_mask = (x.sum(dim=-1) > 0).int()\n",
    "\n",
    "        # 2) Targets: argmax over one-hot; set padding positions to -1\n",
    "        targets = x.argmax(dim=-1)\n",
    "        targets[att_mask == 0] = -1  # padding → -1\n",
    "\n",
    "        # 3) Determine valid maskable positions: real A/C/G/T only\n",
    "        valid = (targets >= 0) & (targets < 4)\n",
    "        n_valid = valid.sum().item()\n",
    "        n_chunks = math.ceil(self.mask_ratio * n_valid / self.chunk_size)\n",
    "\n",
    "        # 4) Sample non-overlapping contiguous spans\n",
    "        starts = []\n",
    "        while len(starts) < n_chunks:\n",
    "            s = random.randrange(0, L - self.chunk_size + 1)\n",
    "            if valid[s] and all(abs(s - p) >= self.chunk_size for p in starts):\n",
    "                starts.append(s)\n",
    "\n",
    "        mask = torch.zeros(L, dtype=torch.bool)\n",
    "        for s in starts:\n",
    "            mask[s : s + self.chunk_size] = True\n",
    "        mask &= valid  # ensure no padding or N masked\n",
    "\n",
    "        # 5) Apply uniform fill (1/5) to masked positions\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask] = 1.0 / 5.0\n",
    "\n",
    "        return x_masked, targets, att_mask, mask\n",
    "    \n",
    "dataset = MaskedOneHotDataset(X)\n",
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len: int,\n",
    "        d_model: int = 768,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # --- Single Conv block + Norm + Dropout + Pool ---\n",
    "        self.conv = nn.Conv1d(5, d_model, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Compute downsampled length\n",
    "        # After conv: ceil(max_len/2), after pool: ceil(prev/2)\n",
    "        L1 = math.ceil(max_len / 2)\n",
    "        L2 = math.ceil(L1 / 2)\n",
    "        self.down_len = L2\n",
    "\n",
    "        # --- Learned positional embeddings ---\n",
    "        self.pos_emb = nn.Embedding(self.down_len, d_model)\n",
    "\n",
    "        # --- BERT-style Transformer ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead-1,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Upsampling via ConvTranspose1d x2 ---\n",
    "        self.up1 = nn.ConvTranspose1d(d_model, d_model, kernel_size=2, stride=2)\n",
    "        self.act_up1 = nn.GELU()\n",
    "        self.up2 = nn.ConvTranspose1d(d_model, d_model, kernel_size=2, stride=2)\n",
    "        self.act_up2 = nn.GELU()\n",
    "\n",
    "        # --- Final classification head over 4 bases ---\n",
    "        self.classifier = nn.Linear(d_model, 4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, att_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, L, 5) one-hot input\n",
    "        att_mask: (B, L) attention mask (1 = valid, 0 = pad)\n",
    "        \"\"\"\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # --- Conv stage ---\n",
    "        h = x.transpose(1, 2)            # (B, 5, L)\n",
    "        h = self.conv(h)                # (B, d_model, L1)\n",
    "        h = h.transpose(1, 2)            # (B, L1, d_model)\n",
    "        h = self.norm(h)\n",
    "        h = F.gelu(h)\n",
    "        h = self.drop(h)\n",
    "        h = h.transpose(1, 2)            # (B, d_model, L1)\n",
    "        h = self.pool(h)                # (B, d_model, down_len)\n",
    "\n",
    "        # --- Add positional embeddings ---\n",
    "        h = h.transpose(1, 2)            # (B, down_len, d_model)\n",
    "        pos = self.pos_emb.weight.unsqueeze(0)  # (1, down_len, d_model)\n",
    "        h = h + pos\n",
    "\n",
    "        # --- Transformer ---\n",
    "        # Downsample attention mask to match down_len\n",
    "        factor = L // self.down_len\n",
    "        att_ds = att_mask[:, ::factor] == 0\n",
    "        z = self.transformer(\n",
    "            h.permute(1, 0, 2),\n",
    "            src_key_padding_mask=att_ds\n",
    "        )                                # (down_len, B, d_model)\n",
    "        z = z.permute(1, 0, 2)           # (B, down_len, d_model)\n",
    "\n",
    "        # --- Upsampling back to original resolution ---\n",
    "        u = z.transpose(1, 2)            # (B, d_model, down_len)\n",
    "        u = self.act_up1(self.up1(u))    # (B, d_model, down_len*2)\n",
    "        u = self.act_up2(self.up2(u))    # (B, d_model, down_len*4)\n",
    "        u = u.transpose(1, 2)            # (B, L_out, d_model)\n",
    "\n",
    "        # --- Classification across 4 bases ---\n",
    "        logits = self.classifier(u)      # (B, L_out, 4)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa22e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 23771908\n",
      "torch.Size([10, 660, 5])\n",
      "tensor([[[ 0.0049, -0.0096,  0.0476,  0.0325],\n",
      "         [ 0.0443, -0.0489,  0.0145,  0.0230],\n",
      "         [ 0.0309, -0.0773,  0.0362,  0.0444],\n",
      "         ...,\n",
      "         [ 0.0180, -0.0442,  0.0173,  0.0157],\n",
      "         [ 0.0036, -0.0191,  0.0598,  0.0135],\n",
      "         [ 0.0674, -0.0493, -0.0094,  0.0099]],\n",
      "\n",
      "        [[ 0.0079, -0.0030,  0.0570,  0.0162],\n",
      "         [ 0.0463, -0.0486,  0.0438,  0.0364],\n",
      "         [ 0.0193, -0.0931,  0.0470,  0.0485],\n",
      "         ...,\n",
      "         [ 0.0086, -0.0357,  0.0365,  0.0338],\n",
      "         [ 0.0301, -0.0172,  0.0117,  0.0206],\n",
      "         [ 0.0446, -0.0377,  0.0238,  0.0149]],\n",
      "\n",
      "        [[ 0.0068, -0.0081,  0.0382,  0.0075],\n",
      "         [ 0.0281, -0.0725,  0.0260,  0.0490],\n",
      "         [ 0.0097, -0.0555,  0.0436,  0.0517],\n",
      "         ...,\n",
      "         [ 0.0259, -0.0437,  0.0361,  0.0388],\n",
      "         [ 0.0242,  0.0009,  0.0133,  0.0317],\n",
      "         [ 0.0477, -0.0406,  0.0132,  0.0199]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0126, -0.0205,  0.0338,  0.0174],\n",
      "         [ 0.0431, -0.0611,  0.0310,  0.0495],\n",
      "         [ 0.0319, -0.0747,  0.0395,  0.0307],\n",
      "         ...,\n",
      "         [ 0.0219, -0.0413,  0.0196,  0.0331],\n",
      "         [ 0.0228, -0.0005,  0.0160,  0.0236],\n",
      "         [ 0.0509, -0.0432,  0.0287,  0.0270]],\n",
      "\n",
      "        [[ 0.0424,  0.0082,  0.0675,  0.0172],\n",
      "         [ 0.0542, -0.0574,  0.0258,  0.0384],\n",
      "         [ 0.0015, -0.0899,  0.0422,  0.0491],\n",
      "         ...,\n",
      "         [ 0.0273, -0.0355,  0.0091,  0.0280],\n",
      "         [ 0.0189, -0.0178,  0.0333,  0.0154],\n",
      "         [ 0.0630, -0.0308,  0.0059,  0.0409]],\n",
      "\n",
      "        [[ 0.0274, -0.0138,  0.0177,  0.0647],\n",
      "         [ 0.0326, -0.0423,  0.0251,  0.0676],\n",
      "         [ 0.0280, -0.1056,  0.0083,  0.0238],\n",
      "         ...,\n",
      "         [ 0.0154, -0.0473,  0.0431,  0.0451],\n",
      "         [ 0.0373, -0.0017,  0.0404,  0.0281],\n",
      "         [ 0.0575, -0.0380,  0.0138,  0.0171]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "model = CNN_MLM(660)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "seq_masked, targets, att_mask, mask = next(iter(loader))\n",
    "print(seq_masked.shape)\n",
    "print(model(seq_masked, att_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4baf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 1e-4,\n",
    "    weight_mask: float = 2.0,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop that:\n",
    "      - takes full-length logits from model(x_masked, att_mask)\n",
    "      - splits masked vs. seen losses (weighted by weight_mask)\n",
    "      - logs per-100-step losses\n",
    "      - uses OneCycleLR + gradient clipping\n",
    "    Assumes:\n",
    "      MaskedOneHotDataset returns (x_masked, targets, att_mask, mask)\n",
    "      with x_masked: (B, L, 5),\n",
    "           targets:  (B, L) in {-1,0,1,2,3,4},\n",
    "           att_mask: (B, L),\n",
    "           mask:     (B, L) Boolean for masked A/C/G/T only.\n",
    "      model(x_masked, att_mask) -> logits (B, L, 4).\n",
    "    \"\"\"\n",
    "    loader = DataLoader(\n",
    "        MaskedOneHotDataset(X, mask_ratio=0.15, chunk_size=4),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    model     = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy=\"linear\",\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss_masked = total_acc_masked = count_masked = 0\n",
    "        total_loss_seen   = total_acc_seen   = count_seen   = 0\n",
    "\n",
    "        for step, (x_masked, targets, att_mask, mask) in enumerate(loader, start=1):\n",
    "            # --- Move to GPU ---\n",
    "            x_masked = x_masked.to(device)    # (B, L, 5)\n",
    "            targets  = targets.to(device)     # (B, L)\n",
    "            att_mask = att_mask.to(device)    # (B, L)\n",
    "            mask      = mask.to(device)       # (B, L)\n",
    "\n",
    "            # --- Forward ---\n",
    "            logits = model(x_masked, att_mask)  # (B, L, 4)\n",
    "            print(logits.shape)\n",
    "\n",
    "            preds = logits.argmax(dim=-1)       # (B, L)\n",
    "\n",
    "            # --- Build masks for loss ---\n",
    "            valid_pos  = (targets >= 0) & (targets < 4)\n",
    "            masked_pos = mask & valid_pos\n",
    "            seen_pos   = (~mask) & valid_pos\n",
    "\n",
    "            # --- Compute losses ---\n",
    "            loss_masked = (\n",
    "                criterion(logits[masked_pos], targets[masked_pos])\n",
    "                if masked_pos.any() else torch.tensor(0.0, device=device)\n",
    "            )\n",
    "            loss_seen = (\n",
    "                criterion(logits[seen_pos], targets[seen_pos])\n",
    "                if seen_pos.any() else torch.tensor(0.0, device=device)\n",
    "            )\n",
    "            loss = weight_mask * loss_masked + loss_seen\n",
    "            print(loss)\n",
    "\n",
    "            # --- Backprop & step ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # --- Accumulate metrics ---\n",
    "            if masked_pos.any():\n",
    "                total_acc_masked  += (preds[masked_pos] == targets[masked_pos]).sum().item()\n",
    "                total_loss_masked += loss_masked.item() * masked_pos.sum().item()\n",
    "                count_masked     += masked_pos.sum().item()\n",
    "\n",
    "            if seen_pos.any():\n",
    "                total_acc_seen  += (preds[seen_pos] == targets[seen_pos]).sum().item()\n",
    "                total_loss_seen += loss_seen.item() * seen_pos.sum().item()\n",
    "                count_seen     += seen_pos.sum().item()\n",
    "\n",
    "            # --- Log every 100 steps ---\n",
    "            if step % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch} Step {step}/{len(loader)} | \"\n",
    "                    f\"Loss_masked: {loss_masked.item():.4f} | \"\n",
    "                    f\"Loss_seen: {loss_seen.item():.4f} | \"\n",
    "                    f\"Total: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # --- Epoch summary ---\n",
    "        avg_loss_masked = total_loss_masked / count_masked if count_masked else 0.0\n",
    "        avg_acc_masked  = 100.0 * total_acc_masked / count_masked if count_masked else 0.0\n",
    "        avg_loss_seen   = total_loss_seen  / count_seen   if count_seen   else 0.0\n",
    "        avg_acc_seen    = 100.0 * total_acc_seen   / count_seen   if count_seen   else 0.0\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs} DONE ➞ \"\n",
    "            f\"Masked Loss: {avg_loss_masked:.4f}, Acc: {avg_acc_masked:.2f}% | \"\n",
    "            f\"Seen Loss: {avg_loss_seen:.4f}, Acc: {avg_acc_seen:.2f}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "# One-liner to launch training:\n",
    "# train(model, X, epochs=10, batch_size=32, lr=1e-4, weight_mask=2.0, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381599ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0745, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0745, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0750, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0737, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0749, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0746, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0744, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0748, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0743, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0745, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0729, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0739, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0746, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0736, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0738, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0738, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0735, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0729, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0726, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0735, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0731, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0734, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0727, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0723, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0728, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0717, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0724, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0729, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0717, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0721, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0717, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0714, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0717, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0721, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0713, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0705, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0711, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0719, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0708, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0711, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0712, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0704, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0708, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0705, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0706, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0698, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0705, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0692, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0698, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0692, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0698, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0688, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0693, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0692, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0691, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0681, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0685, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0681, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0681, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0683, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0674, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0676, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0669, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0670, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0659, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0671, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0660, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0667, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0658, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0652, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0661, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0651, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0650, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0647, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0665, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0663, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0655, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0645, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0645, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0640, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0620, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0635, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0630, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0628, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0626, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0610, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0606, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0589, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0609, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0576, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0580, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0579, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0573, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0582, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0569, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0573, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0538, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0573, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0552, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0556, grad_fn=<AddBackward0>)\n",
      "Epoch 1 Step 100/134 | Loss_masked: 1.3694 | Loss_seen: 1.3709 | Total: 2.0556\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0536, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0540, grad_fn=<AddBackward0>)\n",
      "torch.Size([32, 660, 4])\n",
      "tensor(2.0523, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X, epochs, batch_size, lr, weight_mask, device)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# --- Backprop & step ---\u001b[39;00m\n\u001b[1;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#clip_grad_norm_(model.parameters(), 1.0)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/barcodebert/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/barcodebert/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, X, epochs=10, batch_size=32, lr=1e-4, weight_mask=0.5, device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barcodebert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
