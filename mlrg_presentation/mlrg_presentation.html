<!DOCTYPE html>
<html lang="en"><head>
<script src="mlrg_presentation_files/libs/clipboard/clipboard.min.js"></script>
<script src="mlrg_presentation_files/libs/quarto-html/tabby.min.js"></script>
<script src="mlrg_presentation_files/libs/quarto-html/popper.min.js"></script>
<script src="mlrg_presentation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="mlrg_presentation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="mlrg_presentation_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="mlrg_presentation_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Pablo A. Millan Arias">
  <meta name="dcterms.date" content="2025-07-22">
  <title>“Tips and Tricks” for pre-training DNA foundation models on an academic budget.</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="mlrg_presentation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="mlrg_presentation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="mlrg_presentation_files/libs/revealjs/dist/theme/quarto-53501bf86f92c1215c47ee92987b6862.css">
  <link href="mlrg_presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="mlrg_presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="mlrg_presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="mlrg_presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">“Tips and Tricks” for pre-training DNA foundation models on an academic budget.</h1>
  <p class="subtitle">MLRG Talks &amp; Brainstorms</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Pablo A. Millan Arias <a href="https://orcid.org/0000-0002-8886-9389" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>
<div class="quarto-title-author-email">
<a href="mailto:pmillana[at]uwaterloo[dot]ca">pmillana[at]uwaterloo[dot]ca</a>
</div>
        <p class="quarto-title-affiliation">
            University of Waterloo / Vector Institute
          </p>
    </div>
</div>

  <p class="date">July 22, 2025</p>
</section>
<section id="outline" class="slide level2">
<h2>Outline</h2>
<div style="font-size:0.80em; line-height:1.1; margin:0 1em;">
<ol type="1">
<li>Foundation models in genomics
<ul>
<li><span class="scriptsize">Some examples</span></li>
<li><span class="scriptsize">Different pre-training strategies</span></li>
<li><span class="scriptsize">BarcodeBERT</span></li>
</ul></li>
<li>Motivation (The story of BarcodeMAE)</li>
<li>Engineering Optimizations
<ul>
<li><span class="scriptsize">Mixed precision computation</span></li>
<li><span class="scriptsize">Fused operations</span></li>
</ul></li>
<li>Architectural Optimizations
<ul>
<li><span class="scriptsize">CNN-based tokenization (AlphaGenome)</span></li>
<li><span class="scriptsize">Jumbo CLS and registers</span></li>
</ul></li>
<li>Preliminary Results</li>
<li>Conclusion and Future Work</li>
</ol>
</div>
</section>
<section>
<section id="foundation-models-for-molecular-information" class="title-slide slide level1 center" data-background-color="#40666e">
<h1>Foundation models for molecular information</h1>

</section>
<section id="foundation-models-in-the--omics" class="slide level2">
<h2>Foundation models in the <em>-omics</em></h2>
<p><em>“A <strong>foundation model</strong> is any model that is trained on broad data and can be adapted to a wide range of downstream tasks”</em> <span class="scriptsize">(<em>Bommasani et al., 2021</em>)</span>.</p>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><span class="small">Full understanding of genomic information as a language is particularly hard because every cell has the same “set of instructions” but it differentiates into various cell types</span></li>
<li><span class="small">Every genome is composed of several regions corresponding to <strong>different abstraction levels</strong> (modalities)</span></li>
</ul>
</div><div class="column" style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/central_dogma.png" style="width:300.0%"></p>
<figcaption><span class="tiny">Source: Adapted from <a href="https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/intro-to-gene-expression-central-dogma">Khan Academy</a>.</span></figcaption>
</figure>
</div>
</div></div>
</div>
<div class="fragment">
<p>One of the easiest way of tackling this “problem” was developing one model or each data modality.</p>
</div>
</section>
<section id="foundation-models-in-the--omics-1" class="slide level2">
<h2>Foundation models in the <em>-omics</em></h2>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/Evo_ideal_DNA_Language_Model.png" style="width:50.0%"></p>
<figcaption><span class="tiny text_center">Source: Adapted from (Nguyen et al., 2024)</span></figcaption>
</figure>
</div>
</div>
<ul>
<li class="fragment">Since only ~2% of the genome codes for proteins, several specialized models have been develped the DNA regions whose function is fully understood.</li>
<li class="fragment">Optimal design choices may vary from one “domain” to another.</li>
<li class="fragment">We will focus on the <strong>DNA modality</strong> for the rest of the talk.</li>
</ul>
</section>
<section id="some-examples-of-dna-fundation-models" class="slide level2 scrollable">
<h2>Some examples of DNA fundation models</h2>
<h3 id="many-design-choices-in-dna-foundation-models">Many design Choices in DNA Foundation Models</h3>
<ul>
<li><strong>Tokenization:</strong> k-mers / BPE / single-nucleotide<br>
</li>
<li><strong>Pretraining objectives:</strong> MLM, autoregressive, span-prediction, supervised, distillation<br>
</li>
<li><strong>Architecture:</strong> encoder-only, decoder-only, encoder–decoder, state-space, U-Net, MoE<br>
</li>
<li><strong>Positional encodings:</strong> Sinusoidal, RoPE, ALiBi, Learned</li>
<li><strong>Data domain:</strong> Barcodes, human, multi-species genomes, metagenomes, Fungal ITS<br>
</li>
<li><strong>Model scale:</strong> from ~1 M to &gt;40 B parameters</li>
</ul>
<div class="fragment">
<h3 id="representative-models">Representative Models</h3>
<ul>
<li><strong>HyenaDNA:</strong> <a href="https://arxiv.org/abs/2306.15794" data-preview-link="true">arXiv:2306.15794</a><br>
</li>
<li><strong>MambaDNA:</strong> <a href="https://arxiv.org/abs/2403.03234" data-preview-link="true">arxiv:2403.03234</a><br>
</li>
<li><strong>BarcodeBERT:</strong> <a href="https://arxiv.org/abs/2311.02401" data-preview-link="true">arXiv:2311.02401</a><br>
</li>
<li><strong>BarcodeMamba:</strong> <a href="https://openreview.net/pdf?id=6ohFEFTr10" data-preview-link="true">OpenReview:6ohFEFTr10</a><br>
</li>
<li><strong>Nucleotide Transformer:</strong> <a href="https://www.nature.com/articles/s41592-024-02523-z" data-preview-link="true">Nat. Methods 2024</a><br>
</li>
<li><strong>AlphaGenome:</strong> <a href="https://www.biorxiv.org/content/10.1101/2025.06.25.661532v2" data-preview-link="true">biorxiv:2025.06.25.661532v2</a><br>
</li>
<li><strong>Evo:</strong> <a href="https://arcinstitute.org/tools/evo" data-preview-link="true">Arc Institute</a><br>
</li>
<li><strong>MycoAI:</strong> <a href="https://pubmed.ncbi.nlm.nih.gov/39152642/" data-preview-link="true">PubMed:39152642</a></li>
<li><strong>JanusDNA:</strong> <a href="https://arxiv.org/abs/2505.17257" data-preview-link="true">arXiv:2505.17257</a></li>
</ul>
</div>
</section>
<section id="barcodebert" class="slide level2">
<h2>BarcodeBERT</h2>
<div style="font-size:0.75em; line-height:1.1; margin:0 1em;">
<ul>
<li>BarcodeBERT is a BERT-base transformer model pretrained on ∼1M invertebrate DNA barcodes <span class="small"><em>(Millan Arias et al., 2024)</em></span>.</li>
<li>It models k-mer co-occurrence via masked LM, randomly masking 50 % of tokens and minimizing <span class="math inline">\(\mathcal{L}_{\text{MLM}}=-\sum_{i\in M}\log P(x_i\mid x_{\setminus M}),\)</span> where <span class="math inline">\(M\)</span> is the set of masked-out tokens.</li>
</ul>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/BarcodeBERT_arch.png" style="width:40.0%"></p>
<figcaption><span class="tiny">Source: Adapted from (Millan Arias et al., 2024)</span></figcaption>
</figure>
</div>
</div>
<p>Hidden states serve as features for linear probes or k-NN classification.</p>
</div>
</section></section>
<section>
<section id="motivation-the-story-of-barcodemae" class="title-slide slide level1 center" data-background-color="#40666e">
<h1>Motivation: The story of BarcodeMAE</h1>

</section>
<section id="barcodemae" class="slide level2">
<h2>BarcodeMAE</h2>
<p>Adaptation of MAE (He et al., 2022): Mask 50 % of k-mers, reconstruct via an encoder–decoder architecture</p>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/BarcodeMAE_arch.png" style="width:76.0%"></p>
<figcaption><span class="tiny">Source: Adapted from (Safari et al., 2024)</span></figcaption>
</figure>
</div>
</div><div class="column fragment small" style="text-align:center">
<ul>
<li><strong>Reviewer request:</strong> rerun experiments on a more general (larger) dataset.<br>
</li>
<li><strong>Challenge:</strong> We chose DNABERT-2’s dataset (architecturally similar) but it takes 12 days/train on 4×A40 → unsustainable for iterations<br>
</li>
<li><strong>Takeaway:</strong> We need “tips &amp; tricks” for faster prototyping and efficient pretraining.</li>
<li>I will only consider the GPU that we have “easy” access to <strong><em>(A40, T4, V100, RTX6000)</em></strong>.</li>
</ul>
</div></div>
</div>
</section></section>
<section>
<section id="engineering-optimizations" class="title-slide slide level1 center" data-background-color="#40666e">
<h1>Engineering Optimizations</h1>

</section>
<section id="mixed-precision-training-amp" class="slide level2">
<h2>Mixed Precision Training (AMP)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<h3 class="scrollable" id="floating-point-formats">Floating-Point Formats</h3>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/FP_formats.png" style="width:80.0%"></p>
<figcaption><span class="tiny">Source: Adapted from <a href="https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/">Lighting AI</a></span></figcaption>
</figure>
</div>
</div>
</div><div class="column fragment scriptsize" style="text-align:left">
<p>The algorithm, first introduced by <span class="tiny">(<em>Narang et al., 2024</em>)</span> requires two steps:</p>
<ul>
<li>Porting the model to use the FP16 data type where appropriate.</li>
<li>Adding loss scaling to preserve small gradient values.</li>
</ul>
<p><span class="math display">\[\tilde L = S \times L,\quad g = \tfrac{\partial \tilde L}{S}\]</span></p>
<p>In Pytorch, “automatic mixed precision training” means training with <span class="scriptsize"><code>torch.autocast</code></span> and <span class="scriptsize"><code>torch.amp.GradScaler</code></span> together.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb1-2"><a href=""></a><span class="cf">with</span> autocast():</span>
<span id="cb1-3"><a href=""></a>    loss <span class="op">=</span> model(input_ids, labels<span class="op">=</span>labels).loss</span>
<span id="cb1-4"><a href=""></a>scaler.scale(loss).backward()</span>
<span id="cb1-5"><a href=""></a>scaler.step(optimizer)</span>
<span id="cb1-6"><a href=""></a>scaler.update()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- #!-- Delivers \~2× throughput and \~50 % memory savings on V100/T4. -->
</div></div>
</section>
<section id="mixed-precision-training-amp-1" class="slide level2">
<h2>Mixed Precision Training (AMP)</h2>
<h3 id="performance">Performance</h3>
<p>I investigated the impact of AMP for our model in terms of runtime, prediction accuracy, and memory requirements.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/mixed_precision_benchmark.png.png" style="width:65.0%"></p>
<figcaption><strong>Setup:</strong> PyTorch 2.1.1, single V100 GPU, batch size 512, seq-len 660 nuclotides (DNA barcodes)</figcaption>
</figure>
</div>
</div>
<p>This preliminary results show ~3.2× speedup &amp; ~16% lower peak GPU memory.</p>
</section>
<section id="attention-kernel-implementations" class="slide level2">
<h2>Attention Kernel Implementations</h2>
<h3 id="naive-non-fused">Naive (non-fused)</h3>
<p>GPUs run thousands of threads in lockstep executing the same piece of code (kernel). In deep learning, every op. is implemented as one or more CUDA kernels.</p>
<p><span class="math display">\[\text{Attn}(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d}} \right) V \]</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a>scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)) <span class="op">/</span> math.sqrt(d)</span>
<span id="cb2-2"><a href=""></a>probs  <span class="op">=</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-3"><a href=""></a>y      <span class="op">=</span> torch.matmul(probs, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="fragment">
<p>The previous implementation of attention launches separate kernels for query-key, softmax, and value matmul and stores the entire matrices</p>
</div>
</section>
<section id="attention-kernel-implementations-1" class="slide level2">
<h2>Attention Kernel Implementations</h2>
<h3 id="fused-kernels">Fused kernels</h3>
<p><span class="small">Combine matmul, softmax &amp; dropout in one CUDA pass, cutting activations to <span class="math inline">\(O(L)\)</span> and reducing launch overhead.</span></p>
<div class="fragment scriptsize quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div id="first-column" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Fused kernels are the foundation of modern efficient attention. Specialized domains and new hardware continue to drive custom kernel work.</p>
<ul>
<li><strong>FlashAttention</strong>: Tiling-based, reduce the number of I/O operations but increase FLOPS.</li>
<li><strong>xFormers</strong>: <code>memory_efficient_attention(q,k,v,attn_bias=bias)</code></li>
<li><strong>PyTorch 2.0</strong>: <code>torch.scaled_dot_product_attention</code></li>
<li><strong>Custom kernel design</strong>: DiNO v2 uses custom-made kernels</li>
</ul>
</div>
<div id="second-column" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/Flash_attn.png" style="width:60.0%"></p>
<figcaption><span class="tiny"><strong>Source:</strong> Adapted from (Dao et al., 2023)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="integrating-fused-kernels-to-our-pipeline" class="slide level2 scrolable">
<h2>Integrating fused kernels to our pipeline</h2>
<div class="small">
<ol type="1">
<li class="fragment">Refactoring BarcodeBERT under <a href="https://www.databricks.com/blog/mosaicbert">MosaicBERT</a>’s <span class="small">(<em>Portes et al., 2024</em>)</span> composable architecture (Monireh’s work / Only supported on architectures <span class="math inline">\(\geq\)</span> Ampere).</li>
<li class="fragment">Monkey-patch HuggingFace’s <code>BertSelfAttention</code> using an efficient attention implementation.</li>
</ol>
</div>
<div class="fragment">
<div class="sourceCode" id="cb3" data-code-line-numbers="13-16|25-32|38-43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href=""></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href=""></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href=""></a><span class="im">from</span> transformers <span class="im">import</span> BertForTokenClassification</span>
<span id="cb3-5"><a href=""></a></span>
<span id="cb3-6"><a href=""></a><span class="kw">class</span> FastSelfAttention(nn.Module):</span>
<span id="cb3-7"><a href=""></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb3-8"><a href=""></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-9"><a href=""></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_attention_heads</span>
<span id="cb3-10"><a href=""></a>        <span class="va">self</span>.head_size <span class="op">=</span> config.hidden_size <span class="op">//</span> config.num_attention_heads</span>
<span id="cb3-11"><a href=""></a>        <span class="va">self</span>.all_head_size <span class="op">=</span> config.hidden_size</span>
<span id="cb3-12"><a href=""></a></span>
<span id="cb3-13"><a href=""></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(config.hidden_size, <span class="va">self</span>.all_head_size)</span>
<span id="cb3-14"><a href=""></a>        <span class="va">self</span>.key   <span class="op">=</span> nn.Linear(config.hidden_size, <span class="va">self</span>.all_head_size)</span>
<span id="cb3-15"><a href=""></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(config.hidden_size, <span class="va">self</span>.all_head_size)</span>
<span id="cb3-16"><a href=""></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.attention_probs_dropout_prob)</span>
<span id="cb3-17"><a href=""></a></span>
<span id="cb3-18"><a href=""></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states, attention_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-19"><a href=""></a>        B, L, D <span class="op">=</span> hidden_states.size()</span>
<span id="cb3-20"><a href=""></a>        <span class="co"># project and reshape for multi-head</span></span>
<span id="cb3-21"><a href=""></a>        q <span class="op">=</span> <span class="va">self</span>.query(hidden_states).view(B, L, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_size).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb3-22"><a href=""></a>        k <span class="op">=</span> <span class="va">self</span>.key(hidden_states)  .view(B, L, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_size).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb3-23"><a href=""></a>        v <span class="op">=</span> <span class="va">self</span>.value(hidden_states).view(B, L, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_size).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb3-24"><a href=""></a></span>
<span id="cb3-25"><a href=""></a>        <span class="co"># fused scaled‐dot‐product attention</span></span>
<span id="cb3-26"><a href=""></a>        dp <span class="op">=</span> <span class="va">self</span>.dropout.p <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb3-27"><a href=""></a>        context <span class="op">=</span> F.scaled_dot_product_attention(</span>
<span id="cb3-28"><a href=""></a>            q, k, v,</span>
<span id="cb3-29"><a href=""></a>            attn_mask<span class="op">=</span>attention_mask,</span>
<span id="cb3-30"><a href=""></a>            dropout_p<span class="op">=</span>dp,</span>
<span id="cb3-31"><a href=""></a>            is_causal<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-32"><a href=""></a>        )</span>
<span id="cb3-33"><a href=""></a></span>
<span id="cb3-34"><a href=""></a>        <span class="co"># merge heads</span></span>
<span id="cb3-35"><a href=""></a>        context <span class="op">=</span> context.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>).contiguous().view(B, L, D)</span>
<span id="cb3-36"><a href=""></a>        <span class="cf">return</span> context</span>
<span id="cb3-37"><a href=""></a></span>
<span id="cb3-38"><a href=""></a><span class="kw">class</span> FastBertForTokenClassification(BertForTokenClassification):</span>
<span id="cb3-39"><a href=""></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb3-40"><a href=""></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(config)</span>
<span id="cb3-41"><a href=""></a>        <span class="co"># replace each layer’s self-attention</span></span>
<span id="cb3-42"><a href=""></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.bert.encoder.layer:</span>
<span id="cb3-43"><a href=""></a>            layer.attention.<span class="va">self</span> <span class="op">=</span> FastSelfAttention(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="integrating-fused-kernels-to-our-pipeline-1" class="slide level2">
<h2>Integrating fused kernels to our pipeline</h2>
<h3 id="performance-1">Performance</h3>
<p>I investigated the impact of fused attention Kernels for our model in terms of runtime and memory requirements.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/att_benchmark.png" style="width:80.0%"></p>
<figcaption><strong>Setup:</strong> PyTorch 2.1.1, single V100 / A40 GPU, batch size 512, seq-len 660 nuclotides (DNA barcodes)</figcaption>
</figure>
</div>
</div>
<p>This preliminary result is somewhat expected as fused-kernels’ improvements in other domains ofetn appear for larger inputs and more specialized hardware.</p>
</section></section>
<section>
<section id="architectural-optimizations" class="title-slide slide level1 center" data-background-color="#40666e">
<h1>Architectural Optimizations</h1>

</section>
<section id="a-recent-breakthrough-in-computational-biology-alphagenome" class="slide level2">
<h2>A recent breakthrough in computational biology: <a href="https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/" class="scrollable">AlphaGenome</a></h2>
<p><a href="https://deepmind.google/api/blob/website/media/AlphaGenome-Figure1-Animation-000000.mp4">Demo</a></p>
<div class="fragment">
<p>AlphaGenome (DeepMind, June 2025) is the first unifying DNA sequence model that jointly predicts thousands of regulatory readouts (gene expression, chromatin accessibility, 3D contacts, splicing) from up to <strong>1M nucleotides</strong>.</p>
<blockquote>
<p>“By extending “AlphaGenome” beyond proteins into noncoding DNA, it tackles the genome’s “dark matter” with base-level resolution and long-range context.</p>
</blockquote>
</div>
</section>
<section id="architecture-training" class="slide level2 scrollable">
<h2>Architecture &amp; Training</h2>
<h3 id="architecture-overview">Architecture Overview</h3>
<div class="fragment quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/AlphaGenome_Overview.png" style="width:100.0%"></p>
<figcaption><span class="scriptsize"><strong>Source:</strong> Adapted from (Avsec et al., 2025)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li class="fragment"><strong>Conv encoder</strong> extracts local motifs via stacked Conv1D (<code>DNA embedder(num_channels=768, width=15)</code>).</li>
<li class="fragment"><strong>U-Net pyramid</strong> pools &amp; upsamples for multiscale features.<br>
</li>
<li class="fragment"><strong>Sparse-Attn transformer</strong> uses sliding windows + Rotary positional Encoding + global tokens for long-range.<br>
</li>
<li class="fragment"><strong>Classification heads</strong>: separate heads for each genomic track (regression/class).</li>
</ul>
</div>
</div>
</div>
<div class="fragment" style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/AlphaEvolve_ComprehensiveOverview.png" style="width:40.0%"></p>
<figcaption><span class="scriptsize"><strong>Source:</strong> Adapted from (Avsec et al., 2025)</span></figcaption>
</figure>
</div>
</div>
</section>
<section id="architecture-training-1" class="slide level2">
<h2>Architecture &amp; training</h2>
<h3 id="training">Training</h3>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/Alpha%20Genome_Training.png" style="width:35.0%"></p>
<figcaption><span class="scriptsize"><strong>Source:</strong> Adapted from (Avsec et al., 2025)</span></figcaption>
</figure>
</div>
</div>
<div class="small">
<ul>
<li><strong>Supervised pretrain</strong> on thousands of experimental tracks (ENCODE, GTEx, 4D Nucleome, FANTOM5) covering hundreds of human &amp; mouse cell types.<br>
</li>
<li><strong>Distillation</strong> via held-out cell-type folds to compress and stabilize predictions.</li>
</ul>
</div>
</section>
<section id="idea-cnn-based-encoder" class="slide level2">
<h2>Idea: CNN-Based encoder</h2>
<p>Adapted AlphaGenome’s conv frontend into BarcodeBERT. Vision folks might be familiar with this idea through <span class="scriptsize">(<a href="https://arxiv.org/pdf/2106.14881">Xiao et al, 2021</a>)</span>:</p>
<div class="sourceCode" id="cb4" data-code-line-numbers="13-18|20-25|26-37|39-46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a><span class="kw">class</span> CNN_MLM(nn.Module):</span>
<span id="cb4-2"><a href=""></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-3"><a href=""></a>        <span class="va">self</span>,</span>
<span id="cb4-4"><a href=""></a>        max_len: <span class="bu">int</span>,</span>
<span id="cb4-5"><a href=""></a>        d_model: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span>,</span>
<span id="cb4-6"><a href=""></a>        nhead: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb4-7"><a href=""></a>        num_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb4-8"><a href=""></a>        dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb4-9"><a href=""></a>    ):</span>
<span id="cb4-10"><a href=""></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-11"><a href=""></a>        <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb4-12"><a href=""></a></span>
<span id="cb4-13"><a href=""></a>        <span class="co"># --- Single Conv block + Norm + Dropout + Pool ---</span></span>
<span id="cb4-14"><a href=""></a>        <span class="va">self</span>.conv_1 <span class="op">=</span> nn.Conv1d(<span class="dv">5</span>, d_model, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-15"><a href=""></a>        <span class="va">self</span>.conv_2 <span class="op">=</span> nn.Conv1d(<span class="dv">5</span>, d_model, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-16"><a href=""></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb4-17"><a href=""></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb4-18"><a href=""></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool1d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-19"><a href=""></a></span>
<span id="cb4-20"><a href=""></a>        <span class="co"># Compute downsampled length</span></span>
<span id="cb4-21"><a href=""></a>        <span class="co"># After conv: ceil(max_len/2), after pool: ceil(prev/2)</span></span>
<span id="cb4-22"><a href=""></a>        L1 <span class="op">=</span> math.ceil(max_len <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb4-23"><a href=""></a>        L2 <span class="op">=</span> math.ceil(L1 <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb4-24"><a href=""></a>        <span class="va">self</span>.down_len <span class="op">=</span> L2</span>
<span id="cb4-25"><a href=""></a></span>
<span id="cb4-26"><a href=""></a>        <span class="co"># --- Learned positional embeddings ---</span></span>
<span id="cb4-27"><a href=""></a>        <span class="va">self</span>.pos_emb <span class="op">=</span> nn.Embedding(<span class="va">self</span>.down_len, d_model)</span>
<span id="cb4-28"><a href=""></a></span>
<span id="cb4-29"><a href=""></a>        <span class="co"># --- BERT-style Transformer ---</span></span>
<span id="cb4-30"><a href=""></a>        encoder_layer <span class="op">=</span> nn.TransformerEncoderLayer(</span>
<span id="cb4-31"><a href=""></a>            d_model<span class="op">=</span>d_model,</span>
<span id="cb4-32"><a href=""></a>            nhead<span class="op">=</span>nhead <span class="op">-</span> <span class="dv">1</span>,</span>
<span id="cb4-33"><a href=""></a>            dim_feedforward<span class="op">=</span><span class="dv">4</span> <span class="op">*</span> d_model,</span>
<span id="cb4-34"><a href=""></a>            dropout<span class="op">=</span>dropout,</span>
<span id="cb4-35"><a href=""></a>            activation<span class="op">=</span><span class="st">"gelu"</span>,</span>
<span id="cb4-36"><a href=""></a>        )</span>
<span id="cb4-37"><a href=""></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.TransformerEncoder(encoder_layer, num_layers<span class="op">=</span>num_layers)</span>
<span id="cb4-38"><a href=""></a></span>
<span id="cb4-39"><a href=""></a>        <span class="co"># --- Upsampling via ConvTranspose1d x2 ---</span></span>
<span id="cb4-40"><a href=""></a>        <span class="va">self</span>.up1 <span class="op">=</span> nn.ConvTranspose1d(d_model, d_model, kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-41"><a href=""></a>        <span class="va">self</span>.act_up1 <span class="op">=</span> nn.GELU()</span>
<span id="cb4-42"><a href=""></a>        <span class="va">self</span>.up2 <span class="op">=</span> nn.ConvTranspose1d(d_model, d_model, kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-43"><a href=""></a>        <span class="va">self</span>.act_up2 <span class="op">=</span> nn.GELU()</span>
<span id="cb4-44"><a href=""></a></span>
<span id="cb4-45"><a href=""></a>        <span class="co"># --- Final classification head over 4 bases ---</span></span>
<span id="cb4-46"><a href=""></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(d_model, <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="idea-cnn-based-encoder-1" class="slide level2">
<h2>Idea: CNN-Based encoder</h2>
<h3 id="performance-2">Performance</h3>
<p>Below is a summary of the masked‐language‐modeling (MLM) objective performance when using the CNN tokenizer at different masking ratios:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">Masking Ratio</th>
<th style="text-align: right;">Unseen Accuracy</th>
<th style="text-align: right;">Seen Accuracy</th>
<th style="text-align: right;">kNN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">92 %</td>
<td style="text-align: right;">100 %</td>
<td style="text-align: right;">55 %</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">89 %</td>
<td style="text-align: right;">100 %</td>
<td style="text-align: right;">61 %</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">88 %</td>
<td style="text-align: right;">100 %</td>
<td style="text-align: right;">67 %</td>
</tr>
</tbody>
</table>
<div class="fragment">
<h3 id="limitations">Limitations:</h3>
<ul>
<li>This taks allows for a more aggressive masking ratio, but it does not translate as a performance improvement.</li>
<li>The decoder is currently a naive linear head.</li>
<li>No relative or rotary positional encoding — pure conv + absolute PE.</li>
<li>Training was performed on 1x A40 GPU, so timing is not comparable</li>
</ul>
</div>
</section>
<section id="idea-use-more-global-tokens" class="slide level2">
<h2>Idea: Use (more) global tokens</h2>
<p>Bojanowski et al.&nbsp;(Meta AI, 2022) found that vision transformers’ single <code>[CLS]</code> token can bottleneck global summarization. Leading to the re-purposing of some tokens by the model as registers. Their idea consisted in prepending a series of learnable tokens that the model can use to enocde useful informations.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/Registers.png" style="width:50.0%"></p>
<figcaption><span class="scriptsize"><strong>Source:</strong> Adapted from (Darcet et al., 2024)</span></figcaption>
</figure>
</div>
</div>
</section>
<section id="idea-use-larger-global-tokens" class="slide level2">
<h2>Idea: Use (larger) global tokens</h2>
<h3 id="jumbo-cls">Jumbo CLS</h3>
<div class="fragment scriptsize">
<p>A recent related paper follows-up on that idea and introduced the concept of a <strong>Jumbo</strong> <code>CLS</code> token that acts as <span class="math inline">\(J\)</span> parallel register tokens that each learn a different global perspective.</p>
</div>
<div class="fragment scriptsize">
<p>Define a single <span class="math inline">\(Jd\)</span>-dimensional lerneable parameter and prepend it to inputs. Split it before the MSA to preservedimensionality and flatten the registers to <span class="math inline">\(h\in\mathbb{R}^{Jd}\)</span> to summarize with a dedicated network:</p>
<p><span class="math display">\[
\hat y = \mathrm{MLP}(h)\in\mathbb{R}^{d}
\]</span></p>
</div>
<div class="fragment" style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_img/Jumbo_CLS.png" style="width:60.0%"></p>
<figcaption><span class="scriptsize"><strong>Source:</strong> Adapted from (Fuller et al., 2025)</span></figcaption>
</figure>
</div>
</div>
</section></section>
<section>
<section id="preliminary-results" class="title-slide slide level1 center" data-background-color="#40666e">
<h1>Preliminary Results</h1>

</section>
<section id="summary-of-preliminary-results" class="slide level2">
<h2>Summary of preliminary results</h2>
<table class="caption-top">
<colgroup>
<col style="width: 28%">
<col style="width: 5%">
<col style="width: 11%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 35%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>AMP</th>
<th>Fused Attn</th>
<th>CNN</th>
<th>Peak Ram (GB)</th>
<th>Hardware @ Training Time</th>
<th>kNN (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BarcodeBERT++</td>
<td>✔</td>
<td>✖</td>
<td>✖</td>
<td>21.2</td>
<td>07 hr 04 min @ 1x V100</td>
<td>73.40</td>
</tr>
<tr class="even">
<td></td>
<td>✖</td>
<td>✔</td>
<td>✖</td>
<td>N / A</td>
<td>22 hr 37 min @ 1x V100</td>
<td>71.46</td>
</tr>
<tr class="odd">
<td></td>
<td>✔</td>
<td>✔</td>
<td>✖</td>
<td>N / A</td>
<td>07 hr 44 min @ 1x V100</td>
<td>73.68</td>
</tr>
<tr class="even">
<td></td>
<td>✖</td>
<td>✖</td>
<td>✔</td>
<td>N / A</td>
<td>07 hr 56 min @ 1x A40</td>
<td>67.02</td>
</tr>
<tr class="odd">
<td>BarcodeBERT</td>
<td>✖</td>
<td>✖</td>
<td>✖</td>
<td>25.2</td>
<td>22 hr 50 min @ 1x V100</td>
<td>70.68</td>
</tr>
</tbody>
</table>
</section>
<section id="discusison-future-work" class="slide level2">
<h2>Discusison &amp; Future Work</h2>
<ul>
<li class="fragment"><strong>AMP</strong> delivers ~3× speedup as promised and fused-attention short barcodes are underwhealming.<br>
</li>
<li class="fragment">These results scream refactoring: port to HuggingFace Trainer or Lightning Fabric (or MosaicBERT) to handle AMP, mixed precision, and logging seamlessly.<br>
</li>
<li class="fragment">I was unable to reach the advertised 78.5 % k-NN accuracy on a single V100—indicating either hardware or and hyperparameter sensitivity.<br>
</li>
<li class="fragment"><strong>Next steps</strong>:
<ul>
<li class="fragment">Integrate Jumbo CLS with the CNN tokenizer to shrink model size and speed training.<br>
</li>
<li class="fragment">Implement a mixed-objective encoder–decoder (MAE-style) decoder.<br>
</li>
<li class="fragment">Rerun experiments on the DNABERT-2 dataset for direct comparison.<br>
</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2 scrollable">
<h2>References</h2>
<ul>
<li>Bommasani R. et al., “On the Opportunities and Risks of Foundation Models,” <em>arXiv</em> (2021).<br>
</li>
<li>He K. et al., “Masked Autoencoders Are Scalable Vision Learners,” <em>CVPR</em> (2022).<br>
</li>
<li>Ji Y. et al., “DNABERT-2: a Transformer Model for DNA Sequence,” <em>Bioinformatics</em> (2023).<br>
</li>
<li>Safari M. et al., “BarcodeBERT: Transformers for Biodiversity Analysis,” <em>arXiv</em> (2024).<br>
</li>
<li>Avsec Ž. et al., “AlphaGenome: Advancing Regulatory Variant Effect Prediction,” <em>bioRxiv</em> (2025).<br>
</li>
<li>Dao T. et al., “FlashAttention: Fast and Memory-Efficient Exact Attention,” <em>NeurIPS</em> (2022).<br>
</li>
<li>Press O. et al., “Long Sequence Modeling with ALiBi,” <em>arXiv</em> (2022).<br>
</li>
<li>Su J. et al., “RoFormer: Enhanced Transformer with Rotary Positional Encoding,” <em>arXiv</em> (2021).<br>
</li>
<li>“MosaicBERT: Composable Efficient Transformers,” Databricks Blog (2024).<br>
</li>
<li>“Accelerating Large Language Models with Mixed Precision Techniques,” Lightning AI Blog (2023).<br>
</li>
<li>“torch.cuda.amp,” PyTorch Documentation (2025).</li>
</ul>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="mlrg_presentation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="mlrg_presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="mlrg_presentation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="mlrg_presentation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>